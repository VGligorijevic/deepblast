#!/usr/bin/env python3

import argparse
import os

import torch
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint
from deepblast.trainer import LightningAligner


def main(args):
    print('args', args)
    model = LightningAligner(args)
    # profiler = AdvancedProfiler()

    trainer = Trainer(
        max_epochs=args.epochs,
        gpus=args.gpus,
        num_nodes=args.nodes,
        accumulate_grad_batches=args.grad_accum,
        distributed_backend=args.backend,
        precision=args.precision,
        check_val_every_n_epoch=0.1,
        # profiler=profiler,
        fast_dev_run=True
        # auto_scale_batch_size='power'
    )

    ckpt_path = os.path.join(
        args.output_directory,
        trainer.logger.name,
        f"version_{trainer.logger.version}",
        "checkpoints",
    )
    print(f'{ckpt_path}:', ckpt_path)
    # initialize Model Checkpoint Saver
    checkpoint_callback = ModelCheckpoint(
        filepath=ckpt_path,
        period=1,
        monitor='validation_loss',
        mode='min',
        verbose=True
    )
    trainer.checkpoint_callback = checkpoint_callback
    print('model', model)
    trainer.fit(model)
    # trainer.test()

    # In case this doesn't checkpoint
    torch.save(model.state_dict(),
               args.output_directory + '/model_current.pt')


if __name__ == '__main__':
    parser = argparse.ArgumentParser(add_help=False)
    parser.add_argument('--gpus', type=int, default=None)
    parser.add_argument('--grad-accum', type=int, default=1)
    parser.add_argument('--nodes', type=int, default=1)
    parser.add_argument('--num-workers', type=int, default=1)
    parser.add_argument('--precision', type=int, default=32)
    parser.add_argument('--backend', type=str, default=None)
    # options include ddp_cpu, dp, dpp

    parser = LightningAligner.add_model_specific_args(parser)
    hparams = parser.parse_args()
    main(hparams)
